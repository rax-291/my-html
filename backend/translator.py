"""
Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø§Ø°Ø¬ NLP (Transformers)
"""

from transformers import MarianMTModel, MarianTokenizer
import torch
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===============================================
# Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙØ¶Ù„ Ù„Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©-Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
# ===============================================

MODEL_NAME = "Helsinki-NLP/opus-mt-en-ar"
# Ø¨Ø¯Ø§Ø¦Ù„ Ø¥Ø°Ø§ Ù…Ø§ Ø§Ø´ØªØºÙ„:
# MODEL_NAME = "facebook/nllb-200-distilled-600M"  # Ø£Ù‚ÙˆÙ‰ Ù„ÙƒÙ† Ø£Ø¨Ø·Ø£
# MODEL_NAME = "Helsinki-NLP/opus-mt-tc-big-en-ar" # Ø£ÙƒØ¨Ø± ÙˆØ£Ø¯Ù‚

logger.info(f"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ±Ø¬Ù…Ø©: {MODEL_NAME}")

try:
    tokenizer = MarianTokenizer.from_pretrained(MODEL_NAME)
    model = MarianMTModel.from_pretrained(MODEL_NAME)
    
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… GPU Ø¥Ø°Ø§ Ù…ØªÙˆÙØ±
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device)
    
    logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­ Ø¹Ù„Ù‰ {device}")
except Exception as e:
    logger.error(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
    raise


# ===============================================
# Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
# ===============================================

def translate_single_text(text, use_two_stage=False, max_length=512):
    """
    ØªØ±Ø¬Ù…Ø© Ù†Øµ ÙˆØ§Ø­Ø¯ Ù…Ù† Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    
    Args:
        text (str): Ø§Ù„Ù†Øµ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ
        use_two_stage (bool): Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØ±Ø¬Ù…Ø© Ù…Ø±Ø­Ù„ØªÙŠÙ† (ØªØ¬Ø±ÙŠØ¨ÙŠ)
        max_length (int): Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ
    
    Returns:
        str: Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØªØ±Ø¬Ù… Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©
    """
    try:
        if not text or not text.strip():
            return ""
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
        text = text.strip()
        
        # Tokenization
        inputs = tokenizer(
            text, 
            return_tensors="pt", 
            padding=True, 
            truncation=True, 
            max_length=max_length
        )
        
        # Ù†Ù‚Ù„ Ù„Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # Ø§Ù„ØªØ±Ø¬Ù…Ø©
        with torch.no_grad():
            translated = model.generate(
                **inputs,
                max_length=max_length,
                num_beams=5,           # Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ±Ø¬Ù…Ø© Ø£ÙØ¶Ù„
                early_stopping=True,
                no_repeat_ngram_size=3 # ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
            )
        
        # ÙÙƒ Ø§Ù„ØªØ±Ù…ÙŠØ²
        translation = tokenizer.decode(
            translated[0], 
            skip_special_tokens=True
        )
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªØ±Ø¬Ù…Ø©
        translation = translation.strip()
        
        # ØªØ±Ø¬Ù…Ø© Ù…Ø±Ø­Ù„ØªÙŠÙ† (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        if use_two_stage:
            translation = improve_translation(translation)
        
        return translation
    
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†Øµ: {e}")
        return f"[Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ±Ø¬Ù…Ø©: {str(e)}]"


# ===============================================
# ØªØ±Ø¬Ù…Ø© Ù…Ø­Ø³Ù‘Ù†Ø© (Ù…Ø±Ø­Ù„ØªÙŠÙ†)
# ===============================================

def improve_translation(text):
    """
    ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ©
    (ØªØ¬Ø±ÙŠØ¨ÙŠ - Ù‚Ø¯ Ù„Ø§ ÙŠØ¹Ø·ÙŠ Ù†ØªØ§Ø¦Ø¬ Ø£ÙØ¶Ù„ Ø¯Ø§Ø¦Ù…Ø§Ù‹)
    """
    try:
        # ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ paraphrase Ù‡Ù†Ø§
        # Ø­Ø§Ù„ÙŠØ§Ù‹ Ù†Ø±Ø¬Ø¹ Ø§Ù„Ù†Øµ ÙƒÙ…Ø§ Ù‡Ùˆ
        return text
    except:
        return text


# ===============================================
# ØªØ±Ø¬Ù…Ø© Ù†ØµÙˆØµ Ù…ØªØ¹Ø¯Ø¯Ø© (Batch)
# ===============================================

def translate_texts(texts, use_two_stage=False, batch_size=5):
    """
    ØªØ±Ø¬Ù…Ø© Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¯ÙØ¹ÙŠØ©
    
    Args:
        texts (list): Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
        use_two_stage (bool): Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØ±Ø¬Ù…Ø© Ù…Ø­Ø³Ù‘Ù†Ø©
        batch_size (int): Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ ÙƒÙ„ Ø¯ÙØ¹Ø©
    
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ØªØ±Ø¬Ù…Ø©
    """
    translated_texts = []
    total = len(texts)
    
    logger.info(f"ğŸ”„ Ø¨Ø¯Ø¡ ØªØ±Ø¬Ù…Ø© {total} Ù†Øµ...")
    
    for i in range(0, total, batch_size):
        batch = texts[i:i+batch_size]
        
        for j, text in enumerate(batch):
            current = i + j + 1
            logger.info(f"ğŸ“ ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†Øµ {current}/{total}")
            
            translation = translate_single_text(text, use_two_stage)
            translated_texts.append(translation)
    
    logger.info(f"âœ… Ø§ÙƒØªÙ…Ù„Øª Ø§Ù„ØªØ±Ø¬Ù…Ø©!")
    return translated_texts


# ===============================================
# ØªØ±Ø¬Ù…Ø© Ù…Ø±Ø­Ù„ØªÙŠÙ† (Ù„Ù„ØªÙˆØ§ÙÙ‚)
# ===============================================

def two_stage_translation(text):
    """
    Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù‚Ø¯ÙŠÙ…
    """
    return translate_single_text(text, use_two_stage=True)


# ===============================================
# Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
# ===============================================

if __name__ == "__main__":
    # Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø³ÙŠØ·
    test_text = "I wish I could hug you on my left shoulder"
    
    print(f"ğŸ“¥ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ: {test_text}")
    print(f"ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ±Ø¬Ù…Ø©...")
    
    translation = translate_single_text(test_text)
    
    print(f"ğŸ“¤ Ø§Ù„ØªØ±Ø¬Ù…Ø©: {translation}")